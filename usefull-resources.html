<!DOCTYPE html>
<html>
    <head>
        <title>Scottie McGrath</title>
        <link rel="icon" href="icon16.png" type="image/x-icon">
        <link rel="stylesheet" href="usefull-resources.css">
        
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Lexend:wght@400;700&display=swap" rel="stylesheet">
    </head>
    <body>
        <header>
            <button 
                class="icon-button"
                onClick="location.href='http://scottiemcgrath.ai'"
            >
                <img src="icon.svg" class="nav-icon">
            </button>
            <div class="nav-bar">
                <p class="header-text">
                    Scottie McGrath's hopefully helpfull website to stave off human extinction
                </p>
                <nav class="nav-header">
                    <button 
                        class="active-button"
                        onClick="location.href='http://scottiemcgrath.ai/usefull-resources'"
                    >
                        Usefull Resources
                    </button>
                    <button onClick="location.href='http://scottiemcgrath.ai/current-state-of-ai'">
                        Current State of AI
                    </button>
                    <button onClick="location.href='http://scottiemcgrath.ai/ai-safety-textbook'">
                        AI Safety Textbook
                    </button>
                    <button onClick="location.href='http://scottiemcgrath.ai/blog'">
                        Blog
                    </button>
                    <button onClick="location.href='http://scottiemcgrath.ai/about'">
                        About
                    </button>
                    <button onClick="location.href='http://scottiemcgrath.ai/contact'">
                        Contact
                    </button>
                </nav>
            </div>
        </header>
        <main>
            <p>
                Below, is my curated list of usefull resources for AI Safety and Capabilities research. They are presented in the recommended order of learning, beginning with recommendations for complete beginners and ending with recommendations for active researchers. Some resources are paywalled, so use the resources in the freedom of information section for free access.
            <hr>
            <p>
                Freedom of information:
                <ul>
                    <li>
                        <a
                            href="https://www.sci-hub.st/"
                            target="blank"
                        >
                            Sci-hub:
                        </a>
                         Copy and paste the url of almost any research paper into the search bar for free access.
                    </li>
                    <li>
                        <a
                            href="https://libgen.fun/"
                            target="blank"
                        >
                            Library Genesis:
                        </a>
                        Search for and download almost any book. Z-Library is a mirror of this.
                    </li>
                    <li>
                        <a
                            href="https://www.reddit.com/r/Piracy/wiki/megathread/"
                            target="_blank"
                        >
                            Reddit's Piracy Megathread:
                        </a>
                        Unfortuantely, this is no longer maintained, but it is a lot more complete than my list can be. Look here if my list fails you.
                    </li>
                    <li>
                        <a
                            href="https://github.com/Igglybuff/awesome-piracy/blob/master/readme.md"
                            target="blank"
                        >
                            Igglybuff's Piracy Resources List:
                        </a>
                        An alternative to the reddit thread. As far as I can tell, it is also no longer maintained.
                    </li>
                </ul>
            </p>
            <hr>
            <p>
                AI Safety: 
                <ul>
                    <li>
                        <a
                            href="https://www.youtube.com/watch?v=pYXy-A4siMw"
                            target="_blank"
                        >
                            Robert Miles' YouTube channel:
                        </a>
                        These videos are great. They give easily understandable overviews of many of the most important topics. Here are a few recommended videos to start with:
                        <ul>
                            <li>
                                <a
                                    href="https://www.youtube.com/watch?v=pYXy-A4siMw"
                                    target="_blank"
                                >
                                    Intro to AI Safety:
                                </a>
                                A very basic overview of the main problem.
                            </li>
                            <li>
                                <a
                                    href="https://www.youtube.com/watch?v=nKJlF-olKmg"
                                    target="_blank"
                                >
                                    9 Examples of Specification Gaming:
                                </a>
                                Real world examples of misalignment.
                                <a
                                    href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml"
                                    target="_blank"
                                >
                                    More Examples
                                </a>
                                
                            </li>
                            <li>
                                <a
                                    href="https://www.youtube.com/watch?v=hEUO6pjwFOo"
                                    target="_blank"
                                >
                                    The Orthogonality Thesis:
                                </a>
                                The most important concept.
                            </li>
                            <li>
                                <a
                                    href="https://www.youtube.com/watch?v=ZeecOKBus3Q"
                                    target="_blank"
                                >
                                    Instrumental Convergence:
                                </a>
                                The second most important concept.
                            </li>
                            <li>
                                <a
                                    href="https://www.youtube.com/watch?v=9i1WlcCudpU"
                                    target="_blank"
                                >
                                    10 Reasons to Ignore AI Safety:
                                </a>
                                A response to common immediate criticisms of AI Safety.
                            </li>
                            <li>
                                <a
                                    href="https://www.youtube.com/watch?v=HOJ1NVtlnyQ"
                                    target="_blank"
                                >
                                    Experts' Predictions about the Future of AI:
                                </a>
                                A review of a poll of what machine learning experts think about AI Safety related topics.
                            </li>
                        </ul>
                    </li>
                    <li>
                        <a
                            href="https://en.wikipedia.org/wiki/Human_Compatible"
                            target="_blank"
                        >
                            Human Compatible - Stuart Russell:
                        </a>
                        Details an argument for AI Safety research. Currently the best single resource for AI Safety information.
                    </li>
                    <li>
                        <a
                            href="https://en.wikipedia.org/wiki/The_Elements_of_Moral_Philosophy"
                            target="_blank"
                        >
                            The Elements of Moral Philosophy - James Rachels:
                        </a>
                        The standard ethics textbook for undergrads. If we are going to align an AI, at some point we have to decide which values we are aligning that AI to.
                    </li>
                    <li>
                        <a
                            href="https://en.wikipedia.org/wiki/A_Different_Universe"
                            target="_blank"
                        >
                            A Different Universe - Robert B. Laughlin:
                        </a>
                        Explains the physics concept of emergence, which will hopefully have a big impact on how you view interpretability.
                    </li>
                    <li>
                        <a
                            href="https://www.goodreads.com/book/show/78614.A_Concise_Introduction_to_Logic_with_CD_ROM_"
                            target="_blank"
                        >
                            A Concise Introduction to Logic - Patrick J. Hurley & Lori Watson:
                        </a>
                        The standard introductory textbook to logic. As AI Safety is currently rarely experimental, logic will be your main tool.
                    </li>
                    <li>
                        <a
                            href="https://rohinshah.com/alignment-newsletter/"
                            target="_blank"
                        >
                            The Alignment Newsletter - Rohin Shah:
                        </a>
                        A Newletter detailing current work in AI Safety. Unfortuantely, it is no longer active, but it is still the best resource for undertanding the current state of research.
                    </li>
                    <li>
                        <a
                            href="https://plato.stanford.edu/index.html"
                            target="_blank"
                        >
                            The Stanford Enclyclopedia of Philosophy:
                        </a>
                        Basically Wikipedia, but written by and for philosophers.
                    </li>
                    <li>
                        <a
                            href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"
                            target="_blank"
                        >
                            Superintelligence - Nick Bostrom:
                        </a>
                        Similar to human compatible, but older.
                    </li>
                    <li>
                        <b>Seminal Papers:</b> As problems are solved or discovered, I will link the relevent papers below.
                    </li>
                </ul>
            </p>
            <hr>
            <p>
                AI Capabilities: 
                <ul>
                    <li>
                        <a
                            href="https://www.youtube.com/watch?v=0QczhVg5HaI"
                            target="_blank"
                        >
                            Why Neural Networks can learn (almost) anything - Emergent Garden:
                        </a>
                        A great video explaining why neural networks are useful.
                    </li>
                    <li>
                        <a
                            href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
                            target="_blank"
                        >
                            Neural Networks Playlist - 3Blue1Brown:
                        </a>
                        A Four video series which goes over the maths behind neural networks at a very basic level.
                    </li>
                    <li>
                        <a
                            href="https://www.youtube.com/watch?v=Ijqkc7OLenI"
                            target="_blank"
                        >
                            The Universal Approximation Theorem for neural networks - Michael Nielsen:
                        </a>
                        A video proof of the universal approximation theorem sans caveats.
                    </li>
                    <li>
                        <a
                            href="https://www.youtube.com/watch?v=VMj-3S1tku0"
                            target="_blank"
                        >
                            The spelled-out intro to neural networks and backpropogation: building micrograd - Andrej Karpathy:
                        </a>
                        A video lecture which will take you through the process of building your first neural network.
                    </li>
                    <li>
                        <a
                            href="https://www.asimovinstitute.org/neural-network-zoo/"
                            target="_blank"
                        >
                            The Neural Network Zoo:
                        </a>
                        A mostly complete list of neural network architectures.
                    </li>
                    <li>
                        <a
                            href="https://huggingface.co/"
                            target="_blank"
                        >
                            Hugging Face:
                        </a>
                        The home of machine learning.
                    </li>
                    <li>
                        More Comming Soon...
                    </li>
                </ul>
            </p>
        </main>
    </body>
</html>